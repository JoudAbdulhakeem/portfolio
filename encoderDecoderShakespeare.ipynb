{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zTvEvvhcJbjn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    \"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
        ")\n",
        "#download file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P7qn_ylJce3",
        "outputId": "e1d4f602-7247-4ba8-bd3b-794de127b344"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(f\"Length of text: {len(text)} characters\")\n",
        "#check length of text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUak4M9nJeH-",
        "outputId": "08fd7d91-cff2-4224-a148-f233237ca57b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])\n",
        "#print first 250 characters in text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4taDasjsJgLS",
        "outputId": "ca7a2760-6cfb-495a-c31d-f4528b78e89c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")\n",
        "#how many unique letters we have"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBHRAgYmJh91",
        "outputId": "46d26f73-198f-4168-dde3-770c7028513f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = [\"abcdefg\", \"xyz\"]\n",
        "\n",
        "# TODO 1\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu8YQX8aJkN8",
        "outputId": "4d7f8386-871b-4902-b42b-c95579bdafea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")\n",
        "#create the tf.keras.layers.StringLookup layer:"
      ],
      "metadata": {
        "id": "fuhhQFtyJmH_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids\n",
        "#convert tokens to char ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo9_CNSzJozG",
        "outputId": "d3258692-4aec-42a0-8acb-87958eb53ff8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "DYUoU0aTJqW1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars\n",
        "#join characters back to string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo7VsmoyJsCF",
        "outputId": "73238e14-47e7-4837-a44d-3f9dc30ae8a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOWZGxikJvCn",
        "outputId": "d184a4ce-9233-46aa-ade0-81720f0430bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "0vieE4iBJw1q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2NvrW70JzJu",
        "outputId": "d98045b9-279e-444b-e5cc-dc38a7e97c7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n"
      ],
      "metadata": {
        "id": "5yEiqwSMJ0hF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtjxFTayJ1xI",
        "outputId": "5626c47d-c7e9-48fc-f124-6dfb5f329998"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ],
      "metadata": {
        "id": "HIbaTptwJ3ey"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAqFjvlqJ5kC",
        "outputId": "53d0900d-fb08-4674-cecc-a22fc434b811"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omUP9wMRJ7EN",
        "outputId": "fc1949aa-c8b7-42c7-9e23-6760d5788a6f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "iJFr2ZrtJ8u0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU5OYc9nJ-rE",
        "outputId": "cd7e9582-0136-4cbb-a4a8-932bfb28280c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)\n"
      ],
      "metadata": {
        "id": "YZ8T-gIPKAeB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxpcLaEDKB09",
        "outputId": "1bfcca4d-847c-4e70-93ef-96697f9ec619"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJrcqLr4KDiR",
        "outputId": "eb2c884a-23e3-4bee-9d3f-a4afdbda3534"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "P9QDJlZuKFtp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        # TODO - Create an embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # TODO - Create a GRU layer\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # TODO - Finally connect it with a dense layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        # since we are training a text generation model,\n",
        "        # we use the previous state, in training. If there is no state,\n",
        "        # then we initialize the state\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "7mMsjmZbKHav"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "InnIpJ8tKJm_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5JHTy1NKLBS",
        "outputId": "f98e623b-ee08-49ff-9aed-b91d25cd9de4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzDf_CH8KMW-",
        "outputId": "5da375f0-841b-4301-c36a-3f2414fa5a2f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0], num_samples=1\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "xuxGeAAqKOrW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices\n",
        "#give us at each timestep a prediction of the nexr character index\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN3cDv99KQoS",
        "outputId": "59598fa6-c488-4e98-caeb-e8866b5bad98"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11, 63, 29, 19,  3, 55, 50,  4, 17, 64, 48, 55, 38, 54, 54, 23, 34,\n",
              "       54, 18, 32, 62, 57, 21, 17, 39, 25, 28,  6, 42, 45, 21, 20, 44,  8,\n",
              "       20, 45, 15, 58, 15, 45, 62, 47, 36, 27, 35, 34, 31, 62, 24,  2, 10,\n",
              "       58,  2, 39, 19, 35, 10, 62,  7,  0,  8, 58, 41, 56, 61, 30, 16, 28,\n",
              "       37, 50, 15, 54,  2, 24, 32, 52, 30,  3, 28, 32, 61, 54, 23,  5, 48,\n",
              "       19, 27,  3, 46, 30, 10, 52, 40, 55, 43, 25, 20,  7, 58, 42])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Decode these to see the text predicted by this untrained model\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBeIXIJNKSWJ",
        "outputId": "be3f59cb-381e-4c69-eb7a-aacd167d72b1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' noise.\\n\\nSecond Conspirator:\\nAnd patient fools,\\nWhose children he hath slain, their base throats tea'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\":xPF!pk$DyipYooJUoESwrHDZLO'cfHGe-GfBsBfwhWNVURwK 3s ZFV3w,[UNK]-sbqvQCOXkBo KSmQ!OSvoJ&iFN!gQ3mapdLG,sc\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
      ],
      "metadata": {
        "id": "vDAIS6hjKUEx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \" # (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M21tZC72KVmK",
        "outputId": "00909b7c-9b2d-4126-a4aa-13eaf502abc4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1888475, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \" # (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZppZDx9KXMH",
        "outputId": "4a06771c-e20f-42af-a073-cc5cc8810f80"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1888475, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)\n"
      ],
      "metadata": {
        "id": "TPfmNB-LKZPf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "_VOErtXaKabB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "#use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training.\n",
        "#to get better results train the model more epochs=30"
      ],
      "metadata": {
        "id": "y23ZlHl4Kb9I"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jVmFjlOKdvF",
        "outputId": "f4d96794-a94b-445e-e29c-a05aafce48d3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 6s 13ms/step - loss: 2.6874\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.9746\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 3s 11ms/step - loss: 1.6968\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.5382\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.4413\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 3s 11ms/step - loss: 1.3720\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.3197\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.2745\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.2323\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.1912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "               # Put a -inf at each bad index.\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ],
      "metadata": {
        "id": "bA7udjYaKfIr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
      ],
      "metadata": {
        "id": "1aOcySfNK71K"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPujbwdHK8Ql",
        "outputId": "14ffaf14-73b7-483e-fe81-0b861ec06919"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Though fear'd with his life remorse,\n",
            "I was hope gut my part.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Why will your highness cause mean to take the justors!\n",
            "When cales at home, were we, pet to them forbid!\n",
            "I'll to my foul brother but some for our.\n",
            "And, well, good night!\n",
            "\n",
            "ISABELLA:\n",
            "My sakent comes not?\n",
            "\n",
            "TARWICK:\n",
            "I troke some situde, perid!!\n",
            "\n",
            "HORMENES:\n",
            "I pun untage twenty friends; why not a slumied\n",
            "Well hath a pried of league intended to toes\n",
            "Here too: you are to predit at myself,\n",
            "I put you gentle, would tagether take the freward sort\n",
            "I know in both the friend by our person.\n",
            "\n",
            "First Senator:\n",
            "Then as there were some secret peace\n",
            "To savisive for thy love between the\n",
            "women;\n",
            "She stoos with colours strick our cernarys.\n",
            "'Nopes my heart Prettine of England and thee with\n",
            "the great love, on the fear in our reverment,\n",
            "Or all agree wretched war.\n",
            "\n",
            "Nurse:\n",
            "I say: I'll say it were too fair Somerset,\n",
            "To pluck I have frown'd with silence to my life\n",
            "Even high doing morn at my life, she speaks of meat.\n",
            "Now, by Some is master, I w \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.634331226348877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFcmKN_4K-N8",
        "outputId": "ade2568a-2f12-4243-d4de-160d9d1000b1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nI had as, man, thy mind of that my friend hath constable,\\nYour ochitable, or as a babe, the bloody cattage stoop,\\nAnd numbering stood formy vidowers troul what I hate,\\nGod's enemies are yours.\\nWhatse to the break? By this forget\\nMy earth to clear up that knows my voctures?\\n\\nPage:\\nHere's a help to make you heard of the good dead?\\n\\nHASTINGS:\\nAnd now!\\n\\nPETRUCHIO:\\nA chern,\\ngood mocks into a fool-ma-night.\\n\\nKING RICHARD II:\\nHer father's heat, the king of that Parely\\nEdward, and shall I plet a bed--bring, and not connoce unto a\\ncommon taggers, as thou hast look'd of\\nThat troublest like a thangeters? no doubt,\\nToads three quarter's assians; if they not ride\\nOnly her purpose.\\n\\nSEBASTIAN:\\nAnd Trust, my rogues! at vallet, and with in secrets.\\n\\nBIONDELLO:\\nWhy study, my liege, is in Rome are not what;\\nO, gut at not princes mother on.\\n\\nARIEL:\\nYet; I'll sleep again; be more my cunning law,\\nThat I have ever her and distrequit,\\na pedlit of love in mine arms:\\nWith all then will bless your accent; my\\na\"\n",
            " b\"ROMEO:\\nIf it met them over thus more toward is roin'd you\\nEncourage, pray, she what you repose you so recounted.\\n\\nGLOUCESTER:\\nClifford, wife, where fair is too?\\n\\nESCALUS:\\nIf it were a merry: bear by our noble Lord of Welchman:\\n'Tear poor trusted laid well where we shall come exteem.\\n\\nBAPTISTA:\\n\\nANGELO:\\nGood name, thy cold, and thou art, worthy mother,\\nTo do those sworn, in three was mercy and my frue\\nTo hope it both, no remedy; I come hither to clear it.\\n\\nBENVOLIO:\\nFor who art smile! Come, come, whence thou sacriament?'\\nResceed, receive she sits that whose corours and the prince's cause\\nOf Dakes when you did not rise:\\nAnd, indeed, some, if they are dislance, to seek him,\\nAnd pity us 'twere where you comes too little amazed: 'Good!\\nCome, get us are soothed up sorry and dead.\\n\\nQUEEN MARGARET:\\nPate naturul lady as the pectmary:\\nTherefore, good my lord, and me were as hence.\\n\\nGRUMIO:\\nAy, if your cause well, good folth Coriolanus. Alas!\\nI happy better her off, but hardly for that place of state,\\n\"\n",
            " b\"ROMEO:\\nShe sollow her, like an rey policy:\\nAnd stirs you both her-ealment hath me not, Say.\\n\\nBINEN:\\nAs Sicilia would wounded love.\\n\\nWARWICK:\\nMy master's vail, I can content you of this common.\\n\\nJULIET:\\nO thoughts and fathest sin!\\nThy friend, funator, is your conquerors, then,\\n\\nGRUMIO:\\nO true Prater; none, what, my lord?\\n\\nGLOUCESTER:\\nWhat I am placed commitstation; my progo!\\nYou welcome!\\n\\nMessenger:\\nSuch a villany, I'll buy upon\\nThy regiments, new.\\n\\nBIONDELLO:\\nClarence, like demands, with an end of the head queen him.\\n\\nKING EDWARD IV:\\nSteep and duty with his ears to be the usediest; and,\\nExterphee, take after the sword,\\nAs Brittoo time.\\n\\nCOMINIUS:\\nMethinks I cannot, did unto thee and\\nNoteling doth for her gracious lord,\\nTo overthrow your honour'd power.\\n\\nEDWARD:\\nAy, but yet shall that thy such mans? why, peace!\\n\\nQUEEN:\\nGood quite play, Lord Francis and turn o' thee!\\n\\nARIEL:\\nMy mind thou speech's a neat.\\n\\nBAPTISTA:\\nPost in that I rail! let's ha' no sword;\\nWhere is the creepy throug me with a b\"\n",
            " b\"ROMEO:\\nThis are thought\\nor tarry on the halm renains of order words?\\n\\nGONZALO:\\nLet mine heaven, I come; for Clarence'!\\n\\nGLOUCESTER:\\nUnder, a pardon, land.\\n\\nKING RICHARD III:\\nHow now, what news?\\n\\nMENENIUS:\\nNoble.\\n\\nHERMIONE:\\nAnd where's the midst! look thou must\\nAngel me of their coat: and that my bedward, say\\nhis fire, what makes wive strokes. But earth\\nA thousand head to cause with tewior cannot have.\\n\\nKING RICHARD II:\\nHie thee, my does, a moider shop,\\nHath set a subject wail on her to a maid,\\nTo profose 'longs at Mairary, as thread to-live\\ncannot duke with those that discouts, for stabitions rogue,\\nNor lost you pity chaffing yourselves\\nTo dry or bodgurish who he shall pitcheed\\n\\nYORK:\\nSofter Nothing be the godden queen,\\nBoth with me in thy breath got a deie.\\n\\nMARIANA:\\nWith the justice o' the fears? Or Shameful love,\\nMy malice have pun to thee that they ease\\nThe prettiest and stand on the worship.\\n\\nLADY CAPULET:\\nMadam, come.\\n\\nMENENIUS:\\nI must not stay at me!\\n\\nKING HENRY VI:\\nMarry, sir! here, \"\n",
            " b\"ROMEO:\\nFie, when every one hang I do thou mark'st of manic?\\nThe hermins of my fearful land.\\n\\nROMEO:\\nAy miserable to thee, strike?\\n\\nMERCUTIO:\\nMoward, very well I brook.\\n\\nFirst WatchmAn:\\nGoe tell me, the blossiof and him, were you, stay never all than\\nforth.\\n\\nCATESSBY:\\nGod take order me Oque my trade, to have I hate longing.\\n\\nANGELO:\\nGood fairly shall; and pile pray'd had for die?\\n\\nSecond Citizen:\\nConsider, that thou brawl and expetual, of our brother's\\nHate, I will be virtuous some friends, Overdone from my tent,\\nIs forbund like service of these concience:\\nTheir rosales firet a soldies of thy fortuness;\\nBearing with him, sepilence, and mercy where Coriolanus\\nOnfer'd you as go that earth and lovely us un\\nTo make my leg more wrangly of love;\\n'Rewer, sir; and more wound to coil'd.\\n\\nDUKE OF AUMERLE:\\nFor us, my lord, if she may stain what I deep\\nTo villain to kill her, since you we go, cal to a\\nfor in the crown of Rome thought of sighs;\\nAnd, draws sometome to mine women or no: amen,\\nInto to speak \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.532809257507324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, \"one_step\")\n",
        "one_step_reloaded = tf.saved_model.load(\"one_step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVAvjLZVLB5i",
        "outputId": "d388f5b4-ee1a-487b-ff7f-c585911e54b5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7eeff0794fa0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lgljXwJLFu3",
        "outputId": "bfa4215e-94ef-4bc7-f5a8-c95d6a6d30ee"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Goed Pardar towords, his work! how are you think?\n",
            "\n",
            "BISHOP OF ELIANBA:\n",
            "I' the greater blood, who not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z-nzC0gyLHnR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}